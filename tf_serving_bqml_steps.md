## TensorFlow Serving â€“ BigQuery ML Model Deployment (Local)

This section documents how the exported BigQuery ML model was served locally using **TensorFlow Serving** in a Codespaces environment.

---

### 1. Create the TensorFlow Serving model directory (versioned)
```bash
mkdir -p serving_dir/tip_model/1
```

**Description:**  
TensorFlow Serving requires models to be stored using a versioned directory structure:  
`models/<model_name>/<version>/`.  
Here, version `1` is used for the first deployment of the model.

---

### 2. Copy the exported SavedModel into the version directory
copy the model from GCS (correct bucket + path)
```bash
gsutil -m cp -r \
gs://calm-snowfall-485503-b4-terra-bucket/taxi_ml_model/tip_model \
/tmp/model/
```
Verify the copy worked
```bash
ls -lah /tmp/model
```

Copy into TensorFlow Serving version directory
```bash
cp -r /tmp/model/tip_model/* serving_dir/tip_model/1/
```
Verify:
```bash
ls -lah serving_dir/tip_model/1
```



**Description:**  
This step copies the TensorFlow SavedModel files (`saved_model.pb` and the `variables/` directory) into the versioned folder so TensorFlow Serving can load the model.

---

### 3. Pull the TensorFlow Serving Docker image
```bash
docker pull tensorflow/serving
```

**Description:**  
Downloads the official TensorFlow Serving Docker image, which provides a production-ready REST API for serving machine learning models.

---

### 4. Run TensorFlow Serving with the model mounted
```bash
docker run -p 8501:8501   --mount type=bind,source=$(pwd)/serving_dir/tip_model,target=/models/tip_model   -e MODEL_NAME=tip_model   -t tensorflow/serving &
```

**Description:**  
- Exposes the REST API on port `8501`  
- Mounts the local model directory into the container  
- Registers the model under the name `tip_model`  
- Runs the container in the background  

---

### 5. Verify the model is loaded
```bash
curl http://localhost:8501/v1/models/tip_model
```

**Description:**  
Checks the model status endpoint to confirm that the model is loaded and available for inference.

You can also open the following URL in a browser:
```
http://localhost:8501/v1/models/tip_model

http://127.0.0.1:8501/v1/models/tip_model


```

---

### 6. Send a prediction request
```bash
curl -X POST http://localhost:8501/v1/models/tip_model:predict   -d '{
    "instances": [
      {
        "passenger_count": 1,
        "trip_distance": 12.2,
        "PULocationID": "193",
        "DOLocationID": "264",
        "payment_type": "2",
        "fare_amount": 20.4,
        "tolls_amount": 0.0
      }
    ]
  }'
```

**Description:**  
Sends a sample taxi trip record to the deployed model using the REST API.  
The response contains the predicted `tip_amount` generated by the model.

---

### Summary
In this workflow, a BigQuery ML model is exported as a TensorFlow SavedModel, copied locally, and served using TensorFlow Serving inside a Docker container. The deployed model is exposed through a REST API, enabling real-time predictions via HTTP requests.
